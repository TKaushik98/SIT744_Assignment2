# ============================================================
# Task 4.2 – Custom CIFAR-10 Solution with Regularisation + Augmentation
# ============================================================

import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
import matplotlib.pyplot as plt

# ---------------------------
# 1) Load CIFAR-10
# ---------------------------
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
y_train = y_train.flatten()
y_test = y_test.flatten()

# Normalise
X_train = X_train.astype("float32")/255.0
X_test  = X_test.astype("float32")/255.0

# ---------------------------
# 2) Data augmentation (flip, rotate, colour jitter)
# ---------------------------
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomContrast(0.1)
])

# Mixup function
def mixup(x, y, alpha=0.2):
    batch_size = tf.shape(x)[0]
    lambda_val = tf.random.uniform([], 0, 1)
    index = tf.random.shuffle(tf.range(batch_size))
    mixed_x = lambda_val * x + (1 - lambda_val) * tf.gather(x, index)
    y_onehot = tf.one_hot(y, 10)
    mixed_y = lambda_val * y_onehot + (1 - lambda_val) * tf.gather(y_onehot, index)
    return mixed_x, mixed_y

# Dataset pipeline with Mixup
batch_size = 128
train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(50000).batch(batch_size)
train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))
train_ds = train_ds.map(lambda x, y: mixup(x, y)).prefetch(tf.data.AUTOTUNE)

test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)

# ---------------------------
# 3) Model (Wide CNN with dropout + L2 reg)
# ---------------------------
def build_wide_cnn():
    l2 = regularizers.l2(1e-4)
    model = models.Sequential([
        layers.Conv2D(64, (3,3), padding="same", activation="relu", kernel_regularizer=l2, input_shape=(32,32,3)),
        layers.Conv2D(64, (3,3), padding="same", activation="relu", kernel_regularizer=l2),
        layers.MaxPooling2D((2,2)),
        layers.Dropout(0.3),

        layers.Conv2D(128, (3,3), padding="same", activation="relu", kernel_regularizer=l2),
        layers.Conv2D(128, (3,3), padding="same", activation="relu", kernel_regularizer=l2),
        layers.MaxPooling2D((2,2)),
        layers.Dropout(0.4),

        layers.Flatten(),
        layers.Dense(256, activation="relu", kernel_regularizer=l2),
        layers.Dropout(0.5),
        layers.Dense(10, activation="softmax")
    ])
    return model

model = build_wide_cnn()

# Optimiser with cosine decay LR
initial_lr = 1e-3
lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_lr, decay_steps=10*len(train_ds))
opt = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

model.compile(optimizer=opt, loss="categorical_crossentropy", metrics=["accuracy"])

# ---------------------------
# 4) Training
# ---------------------------
history = model.fit(train_ds, validation_data=test_ds, epochs=20)

# ---------------------------
# 5) Plot curves
# ---------------------------
plt.plot(history.history["accuracy"], label="Train Acc")
plt.plot(history.history["val_accuracy"], label="Val Acc")
plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.legend()
plt.title("Task 4.2 – Accuracy Curves")
plt.show()
